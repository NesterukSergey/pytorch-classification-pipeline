{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix, Precision, Recall\n",
    "from ignite.contrib.metrics import ROC_AUC\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ImportError:\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\n",
    "            \"This module requires either tensorboardX or torch >= 1.2.0. \"\n",
    "            \"You may install tensorboardX with command: \\n pip install tensorboardX \\n\"\n",
    "            \"or upgrade PyTorch using your package manager of choice (pip or conda).\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*6*6,600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = x.view(-1,64*6*6)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer(model, data_loader, log_dir):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    x, y = next(data_loader_iter)\n",
    "    try:\n",
    "        writer.add_graph(model, x)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save model graph: {}\".format(e))\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_run_name(log_dir):\n",
    "    if not os.path.isdir(log_dir):\n",
    "        return os.path.join(log_dir, 'run_0')\n",
    "    else:\n",
    "        runs = [int(d[4:]) for d in next(os.walk(log_dir))[1] if d[:3] == 'run']\n",
    "        return os.path.join(log_dir, 'run_' + str(max(runs) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activated_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cm(cm, filename='', classes=[]):\n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(cm, cmap='Oranges')\n",
    "    \n",
    "    if len(classes) == 0:\n",
    "        classes = [str(c) for c in range(len(cm))]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    \n",
    "    ax.set_xlabel('Predicted', fontsize=7)\n",
    "    ax.set_xticks(tick_marks)\n",
    "    c = ax.set_xticklabels(classes, fontsize=7, rotation=-90,  ha='center')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('True Label', fontsize=7)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=7, va ='center')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    ax.yaxis.tick_left()\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], 'd') if cm[i,j]!=0 else '.', horizontalalignment=\"center\", fontsize=6, verticalalignment='center', color=\"black\")\n",
    "    fig.set_tight_layout(True)\n",
    "    plt.savefig(os.path.join(filename, 'confusion_matrix.jpg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    dataset_name = 'MNIST'\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
    "    train_loader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    validationset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)\n",
    "    val_loader = DataLoader(validationset, batch_size=val_batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(params):\n",
    "    train_loader, val_loader, dataset_name = get_data_loaders(params['train_batch_size'], params['val_batch_size'])\n",
    "    model_dir = os.path.join(params['log_dir'], dataset_name)\n",
    "    run_dir = get_next_run_name(model_dir)\n",
    "    \n",
    "    model = params['model']\n",
    "    \n",
    "    writer = create_summary_writer(model, train_loader, run_dir)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
    "    criterion = params['criterion']\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=params['lr_patience'], factor=params['lr_factor'], min_lr=params['min_lr'])\n",
    "    \n",
    "    model_name = (\n",
    "        params['model_name'] + \n",
    "        '_lr:' + str(params['lr']) +\n",
    "        '_' + str(optimizer).split(' ')[0] +\n",
    "        '_' + str(criterion).split('.')[-1].split(\"'\")[0][:-2]\n",
    "    )\n",
    "    \n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    \n",
    "    train_evaluator = create_supervised_evaluator(\n",
    "        model, \n",
    "        metrics=\n",
    "        {\n",
    "            \"accuracy\": Accuracy(), \n",
    "            \"nll\": Loss(F.nll_loss),\n",
    "        },\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_evaluator = create_supervised_evaluator(\n",
    "        model, \n",
    "        metrics=\n",
    "        {\n",
    "            \"accuracy\": Accuracy(), \n",
    "            \"nll\": Loss(F.nll_loss),\n",
    "            \"precision\": Precision(),\n",
    "            \"recall\": Recall(),\n",
    "            \"conf_matrix\": ConfusionMatrix(10),\n",
    "        },\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    def score_function(engine):\n",
    "        val_loss = engine.state.metrics['nll']\n",
    "        return -val_loss\n",
    "    \n",
    "    if params['early_stopping']:\n",
    "        handler = EarlyStopping(patience=params['early_stopping_patience'], score_function=score_function, trainer=trainer)\n",
    "        val_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(os.path.join(model_dir,'saved_models'), dataset_name, n_saved=3, create_dir=True, save_as_state_dict=True, require_empty=False)\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {model_name: model})\n",
    "    \n",
    "    def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=params['log_interval']))\n",
    "    def log_training_loss(engine):\n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"training/batch_loss\", engine.state.output, engine.state.iteration)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "                \"\".format(engine.state.epoch, engine.state.iteration, len(train_loader), engine.state.output)\n",
    "            )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        train_evaluator.run(train_loader)\n",
    "        metrics = train_evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        \n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"training/epoch_loss\", avg_nll, engine.state.epoch)\n",
    "#             writer.add_scalar(\"training/epoch_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "                    engine.state.epoch, avg_accuracy, avg_nll\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        val_evaluator.run(val_loader)\n",
    "        metrics = val_evaluator.state.metrics\n",
    "        accuracy = metrics[\"accuracy\"]       \n",
    "        precision = metrics[\"precision\"]\n",
    "        recall = metrics[\"recall\"]\n",
    "        \n",
    "        nll = metrics[\"nll\"] \n",
    "        if scheduler:\n",
    "            scheduler.step(nll)\n",
    "        \n",
    "        save_cm(metrics[\"conf_matrix\"] , filename=run_dir, classes=[])\n",
    "        \n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"valdation/epoch_loss\", nll, engine.state.epoch)\n",
    "            writer.add_scalar(\"valdation/epoch_accuracy\", accuracy, engine.state.epoch)            \n",
    "            writer.add_scalar(\"valdation/epoch_precision\", precision.sum() / len(precision), engine.state.epoch)\n",
    "            writer.add_scalar(\"valdation/epoch_recall\", recall.sum() / len(recall), engine.state.epoch)\n",
    "            \n",
    "            writer.add_scalar(\"valdation/learning_rate\", get_lr(optimizer), engine.state.epoch)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Validation Results - Epoch: {}  Accuracy: {:.2f}  loss: {:.2f}\".format(\n",
    "                    engine.state.epoch, accuracy, nll\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    trainer.run(train_loader, max_epochs=params['epochs'])\n",
    "    writer.close()    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_batch_size': 64,\n",
    "    'val_batch_size': 64,\n",
    "    'epochs': 40,\n",
    "    'lr': 1e-3,\n",
    "    'verbose': False,\n",
    "    'tensorboard': True,\n",
    "    'log_interval': 100,\n",
    "    'log_dir': './tensorboard_logs',\n",
    "    'model_name': 'LeNet',\n",
    "    'model': CNN(),\n",
    "    'criterion': nn.NLLLoss(),\n",
    "    'optimizer': optim.Adam,\n",
    "    'early_stopping': True,\n",
    "    'early_stopping_patience': 11,\n",
    "    'lr_patience': 5,\n",
    "    'lr_factor': 0.25,\n",
    "    'min_lr': 1e-5,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./tensorboard_logs/MNIST/saved_models/MNIST_LeNet_lr:0.001_Adam_NLLLoss_13132.pth'))\n",
    "\n",
    "params = {\n",
    "    'train_batch_size': 64,\n",
    "    'val_batch_size': 64,\n",
    "    'epochs': 40,\n",
    "    'lr': 1e-4,\n",
    "    'verbose': False,\n",
    "    'tensorboard': True,\n",
    "    'log_interval': 100,\n",
    "    'log_dir': './tensorboard_logs',\n",
    "    'model_name': 'LeNet',\n",
    "    'model': model,\n",
    "    'criterion': nn.NLLLoss(),\n",
    "    'optimizer': optim.Adam,\n",
    "    'early_stopping': True,\n",
    "    'early_stopping_patience': 11,\n",
    "    'lr_patience': 5,\n",
    "    'lr_factor': 0.25,\n",
    "    'min_lr': 1e-5,\n",
    "}\n",
    "\n",
    "model = run(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classes of fashion mnist dataset\n",
    "# classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "# # creating iterator for iterating the dataset\n",
    "# dataiter = iter(val_loader)\n",
    "# images, labels = dataiter.next()\n",
    "# images_arr = []\n",
    "# labels_arr = []\n",
    "# pred_arr = []\n",
    "# # moving model to cpu for inference \n",
    "# model.to(\"cpu\")\n",
    "# # iterating on the dataset to predict the output\n",
    "# for i in range(0,10):\n",
    "#     images_arr.append(images[i].unsqueeze(0))\n",
    "#     labels_arr.append(labels[i].item())\n",
    "#     ps = torch.exp(model(images_arr[i]))\n",
    "#     ps = ps.data.numpy().squeeze()\n",
    "#     pred_arr.append(np.argmax(ps))\n",
    "# # plotting the results\n",
    "# fig = plt.figure(figsize=(25,4))\n",
    "# for i in range(10):\n",
    "#     ax = fig.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
    "#     ax.imshow(images_arr[i].resize_(1, 28, 28).numpy().squeeze())\n",
    "#     ax.set_title(\"{} ({})\".format(classes[pred_arr[i]], classes[labels_arr[i]]),\n",
    "#                  color=(\"green\" if pred_arr[i]==labels_arr[i] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
