{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except ImportError:\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\n",
    "            \"This module requires either tensorboardX or torch >= 1.2.0. \"\n",
    "            \"You may install tensorboardX with command: \\n pip install tensorboardX \\n\"\n",
    "            \"or upgrade PyTorch using your package manager of choice (pip or conda).\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*6*6,600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = x.view(-1,64*6*6)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer(model, data_loader, log_dir):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    data_loader_iter = iter(data_loader)\n",
    "    x, y = next(data_loader_iter)\n",
    "    try:\n",
    "        writer.add_graph(model, x)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save model graph: {}\".format(e))\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_run_name(log_dir):\n",
    "    if not os.path.isdir(log_dir):\n",
    "        return os.path.join(log_dir, 'run_0')\n",
    "    else:\n",
    "        runs = [int(d[4:]) for d in next(os.walk(log_dir))[1] if d[:3] == 'run']\n",
    "        return os.path.join(log_dir, 'run_' + str(max(runs) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(params):\n",
    "    run_dir = get_next_run_name(params['log_dir'])\n",
    "    train_loader, val_loader = get_data_loaders(params['train_batch_size'], params['val_batch_size'])\n",
    "    model = params['model']\n",
    "    \n",
    "    writer = create_summary_writer(model, train_loader, run_dir)\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
    "    criterion = params['criterion']\n",
    "    \n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    evaluator = create_supervised_evaluator(\n",
    "        model, metrics={\"accuracy\": Accuracy(), \"nll\": Loss(F.nll_loss)}, device=device\n",
    "    )\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=params['log_interval']))\n",
    "    def log_training_loss(engine):\n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"training/loss\", engine.state.output, engine.state.iteration)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "                \"\".format(engine.state.epoch, engine.state.iteration, len(train_loader), engine.state.output)\n",
    "            )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        \n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"training/avg_loss\", avg_nll, engine.state.epoch)\n",
    "            writer.add_scalar(\"training/avg_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "                    engine.state.epoch, avg_accuracy, avg_nll\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        \n",
    "        if params['tensorboard']:\n",
    "            writer.add_scalar(\"valdation/avg_loss\", avg_nll, engine.state.epoch)\n",
    "            writer.add_scalar(\"valdation/avg_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "        \n",
    "        if params['verbose']:\n",
    "            print(\n",
    "                \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(\n",
    "                    engine.state.epoch, avg_accuracy, avg_nll\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    trainer.run(train_loader, max_epochs=params['epochs'])\n",
    "    writer.close()    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
    "    train_loader = DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    # Download and load the test data\n",
    "    validationset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)\n",
    "    val_loader = DataLoader(validationset, batch_size=val_batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_batch_size': 64,\n",
    "    'val_batch_size': 64,\n",
    "    'epochs': 10,\n",
    "    'lr': 0.001,\n",
    "    'verbose': True,\n",
    "    'tensorboard': True,\n",
    "    'log_interval': 100,\n",
    "    'log_dir': './tensorboard_logs',\n",
    "    'model': CNN(),\n",
    "    'criterion': nn.NLLLoss(),\n",
    "    'optimizer': optim.Adam,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[100/938] Loss: 0.76\n",
      "Epoch[1] Iteration[200/938] Loss: 0.45\n",
      "Epoch[1] Iteration[300/938] Loss: 0.37\n",
      "Epoch[1] Iteration[400/938] Loss: 0.60\n",
      "Epoch[1] Iteration[500/938] Loss: 0.39\n",
      "Epoch[1] Iteration[600/938] Loss: 0.29\n",
      "Epoch[1] Iteration[700/938] Loss: 0.30\n",
      "Epoch[1] Iteration[800/938] Loss: 0.31\n",
      "Epoch[1] Iteration[900/938] Loss: 0.49\n",
      "Training Results - Epoch: 1  Avg accuracy: 0.90 Avg loss: 0.27\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.89 Avg loss: 0.31\n",
      "Epoch[2] Iteration[1000/938] Loss: 0.18\n",
      "Epoch[2] Iteration[1100/938] Loss: 0.26\n",
      "Epoch[2] Iteration[1200/938] Loss: 0.23\n",
      "Epoch[2] Iteration[1300/938] Loss: 0.40\n",
      "Epoch[2] Iteration[1400/938] Loss: 0.26\n",
      "Epoch[2] Iteration[1500/938] Loss: 0.39\n",
      "Epoch[2] Iteration[1600/938] Loss: 0.18\n",
      "Epoch[2] Iteration[1700/938] Loss: 0.34\n",
      "Epoch[2] Iteration[1800/938] Loss: 0.26\n",
      "Training Results - Epoch: 2  Avg accuracy: 0.91 Avg loss: 0.26\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.89 Avg loss: 0.30\n",
      "Epoch[3] Iteration[1900/938] Loss: 0.21\n",
      "Epoch[3] Iteration[2000/938] Loss: 0.21\n",
      "Epoch[3] Iteration[2100/938] Loss: 0.29\n",
      "Epoch[3] Iteration[2200/938] Loss: 0.31\n",
      "Epoch[3] Iteration[2300/938] Loss: 0.15\n",
      "Epoch[3] Iteration[2400/938] Loss: 0.19\n",
      "Epoch[3] Iteration[2500/938] Loss: 0.21\n",
      "Epoch[3] Iteration[2600/938] Loss: 0.30\n",
      "Epoch[3] Iteration[2700/938] Loss: 0.41\n",
      "Epoch[3] Iteration[2800/938] Loss: 0.19\n",
      "Training Results - Epoch: 3  Avg accuracy: 0.91 Avg loss: 0.24\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.89 Avg loss: 0.31\n",
      "Epoch[4] Iteration[2900/938] Loss: 0.15\n",
      "Epoch[4] Iteration[3000/938] Loss: 0.15\n",
      "Epoch[4] Iteration[3100/938] Loss: 0.28\n",
      "Epoch[4] Iteration[3200/938] Loss: 0.10\n",
      "Epoch[4] Iteration[3300/938] Loss: 0.14\n",
      "Epoch[4] Iteration[3400/938] Loss: 0.21\n",
      "Epoch[4] Iteration[3500/938] Loss: 0.25\n",
      "Epoch[4] Iteration[3600/938] Loss: 0.28\n",
      "Epoch[4] Iteration[3700/938] Loss: 0.28\n",
      "Training Results - Epoch: 4  Avg accuracy: 0.91 Avg loss: 0.23\n",
      "Validation Results - Epoch: 4  Avg accuracy: 0.89 Avg loss: 0.31\n",
      "Epoch[5] Iteration[3800/938] Loss: 0.32\n",
      "Epoch[5] Iteration[3900/938] Loss: 0.33\n",
      "Epoch[5] Iteration[4000/938] Loss: 0.14\n",
      "Epoch[5] Iteration[4100/938] Loss: 0.19\n",
      "Epoch[5] Iteration[4200/938] Loss: 0.33\n",
      "Epoch[5] Iteration[4300/938] Loss: 0.23\n",
      "Epoch[5] Iteration[4400/938] Loss: 0.16\n",
      "Epoch[5] Iteration[4500/938] Loss: 0.45\n",
      "Epoch[5] Iteration[4600/938] Loss: 0.17\n",
      "Training Results - Epoch: 5  Avg accuracy: 0.93 Avg loss: 0.19\n",
      "Validation Results - Epoch: 5  Avg accuracy: 0.91 Avg loss: 0.27\n",
      "Epoch[6] Iteration[4700/938] Loss: 0.20\n",
      "Epoch[6] Iteration[4800/938] Loss: 0.09\n",
      "Epoch[6] Iteration[4900/938] Loss: 0.17\n",
      "Epoch[6] Iteration[5000/938] Loss: 0.21\n",
      "Epoch[6] Iteration[5100/938] Loss: 0.15\n",
      "Epoch[6] Iteration[5200/938] Loss: 0.24\n",
      "Epoch[6] Iteration[5300/938] Loss: 0.09\n",
      "Epoch[6] Iteration[5400/938] Loss: 0.25\n",
      "Epoch[6] Iteration[5500/938] Loss: 0.14\n",
      "Epoch[6] Iteration[5600/938] Loss: 0.16\n",
      "Training Results - Epoch: 6  Avg accuracy: 0.95 Avg loss: 0.15\n",
      "Validation Results - Epoch: 6  Avg accuracy: 0.91 Avg loss: 0.26\n",
      "Epoch[7] Iteration[5700/938] Loss: 0.11\n",
      "Epoch[7] Iteration[5800/938] Loss: 0.17\n",
      "Epoch[7] Iteration[5900/938] Loss: 0.12\n",
      "Epoch[7] Iteration[6000/938] Loss: 0.13\n",
      "Epoch[7] Iteration[6100/938] Loss: 0.32\n",
      "Epoch[7] Iteration[6200/938] Loss: 0.22\n",
      "Epoch[7] Iteration[6300/938] Loss: 0.16\n",
      "Epoch[7] Iteration[6400/938] Loss: 0.15\n",
      "Epoch[7] Iteration[6500/938] Loss: 0.14\n",
      "Training Results - Epoch: 7  Avg accuracy: 0.94 Avg loss: 0.16\n",
      "Validation Results - Epoch: 7  Avg accuracy: 0.91 Avg loss: 0.28\n",
      "Epoch[8] Iteration[6600/938] Loss: 0.13\n",
      "Epoch[8] Iteration[6700/938] Loss: 0.15\n",
      "Epoch[8] Iteration[6800/938] Loss: 0.19\n",
      "Epoch[8] Iteration[6900/938] Loss: 0.20\n",
      "Epoch[8] Iteration[7000/938] Loss: 0.07\n",
      "Epoch[8] Iteration[7100/938] Loss: 0.23\n",
      "Epoch[8] Iteration[7200/938] Loss: 0.16\n",
      "Epoch[8] Iteration[7300/938] Loss: 0.39\n",
      "Epoch[8] Iteration[7400/938] Loss: 0.12\n",
      "Epoch[8] Iteration[7500/938] Loss: 0.07\n",
      "Training Results - Epoch: 8  Avg accuracy: 0.95 Avg loss: 0.14\n",
      "Validation Results - Epoch: 8  Avg accuracy: 0.91 Avg loss: 0.28\n",
      "Epoch[9] Iteration[7600/938] Loss: 0.15\n",
      "Epoch[9] Iteration[7700/938] Loss: 0.30\n",
      "Epoch[9] Iteration[7800/938] Loss: 0.14\n",
      "Epoch[9] Iteration[7900/938] Loss: 0.22\n",
      "Epoch[9] Iteration[8000/938] Loss: 0.11\n",
      "Epoch[9] Iteration[8100/938] Loss: 0.23\n",
      "Epoch[9] Iteration[8200/938] Loss: 0.19\n",
      "Epoch[9] Iteration[8300/938] Loss: 0.09\n",
      "Epoch[9] Iteration[8400/938] Loss: 0.22\n",
      "Training Results - Epoch: 9  Avg accuracy: 0.96 Avg loss: 0.13\n",
      "Validation Results - Epoch: 9  Avg accuracy: 0.91 Avg loss: 0.27\n",
      "Epoch[10] Iteration[8500/938] Loss: 0.06\n",
      "Epoch[10] Iteration[8600/938] Loss: 0.17\n",
      "Epoch[10] Iteration[8700/938] Loss: 0.09\n",
      "Epoch[10] Iteration[8800/938] Loss: 0.20\n",
      "Epoch[10] Iteration[8900/938] Loss: 0.09\n",
      "Epoch[10] Iteration[9000/938] Loss: 0.24\n",
      "Epoch[10] Iteration[9100/938] Loss: 0.13\n",
      "Epoch[10] Iteration[9200/938] Loss: 0.07\n",
      "Epoch[10] Iteration[9300/938] Loss: 0.16\n",
      "Training Results - Epoch: 10  Avg accuracy: 0.96 Avg loss: 0.12\n",
      "Validation Results - Epoch: 10  Avg accuracy: 0.91 Avg loss: 0.28\n"
     ]
    }
   ],
   "source": [
    "model = run(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classes of fashion mnist dataset\n",
    "# classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "# # creating iterator for iterating the dataset\n",
    "# dataiter = iter(val_loader)\n",
    "# images, labels = dataiter.next()\n",
    "# images_arr = []\n",
    "# labels_arr = []\n",
    "# pred_arr = []\n",
    "# # moving model to cpu for inference \n",
    "# model.to(\"cpu\")\n",
    "# # iterating on the dataset to predict the output\n",
    "# for i in range(0,10):\n",
    "#     images_arr.append(images[i].unsqueeze(0))\n",
    "#     labels_arr.append(labels[i].item())\n",
    "#     ps = torch.exp(model(images_arr[i]))\n",
    "#     ps = ps.data.numpy().squeeze()\n",
    "#     pred_arr.append(np.argmax(ps))\n",
    "# # plotting the results\n",
    "# fig = plt.figure(figsize=(25,4))\n",
    "# for i in range(10):\n",
    "#     ax = fig.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
    "#     ax.imshow(images_arr[i].resize_(1, 28, 28).numpy().squeeze())\n",
    "#     ax.set_title(\"{} ({})\".format(classes[pred_arr[i]], classes[labels_arr[i]]),\n",
    "#                  color=(\"green\" if pred_arr[i]==labels_arr[i] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
